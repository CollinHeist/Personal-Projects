{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to load the two accompanying files.\n",
    "\n",
    "coordinate_data.pickle contains the coordinates of 4 points in 3D space (coord_X), and the volume of the solid formed by using those 4 points as vertices (coord_y).\n",
    "NOTE: coord_X has already been centered about the origin, and then reshaped into (N, 12) so it is ready to use in the \"k hidden layers\" class.\n",
    "\n",
    "distance_data.pickle contains the 6 pairwise distances of 4 points in 3D space (dist_X), and the volume of the solid formed by using those 4 points as vertices (dist_y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def pickle_load(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        contents = pickle.load(f)\n",
    "        return contents\n",
    "    return contents\n",
    "\n",
    "# the network will throw errors without the reshaping of y\n",
    "coord_X, coord_y = pickle_load('coordinate_data.pickle')\n",
    "coord_y = np.reshape(coord_y, (-1, 1))\n",
    "dist_X, dist_y = pickle_load('distance_data.pickle')\n",
    "dist_y = np.reshape(dist_y, (-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "As always, rename this file to include your first and last name before submitting.\n",
    "\n",
    "Split up the data into a training set and validation set (use 1% of the data for validation)\n",
    "\n",
    "Use the k_hidden_layer class that Frank posted to train two neural networks: one that can predict the volume of the solid based on the coordinate data, and another that can predict the volume based on the pairwise distances.\n",
    "\n",
    "The current setup of k_hidden_layer is using the crossentropy loss function, which works well for classification (like on MNIST), but does not make sense for for a regression problem like this one (you should make sure you understand why this is by looking at how crossentropy is formulated).  You will need to rewrite that part of the code to implement the mean squared error (MSE) loss function defined below.  There are three parts you need to edit: the gradient calculation, the loss, and you need to print the loss instead of accuracy.\n",
    "\n",
    "$$ \\text{MSE Loss:} L = \\frac{1}{n} \\sum_{i=1}^{n} (y_{i, pred} - y_{i, true})^2 \\hspace{1 cm} \\text{(where n is the batch size)}$$\n",
    "\n",
    "NOTE: To get the training to work well you will need to also look at the distribution of y (see the code below).  One of the most important data preprocessing steps is making sure your labels are spread out evenly (Why?).  The ideal is a uniform distribution, but a normal distribution is often good enough.  To get y to have a (roughly) uniform distribution you can use a one-to-one function like log, or nth root on y for training (or a combination of one-to-one functions), then apply the inverse of the function on your predictions when you want to make a prediction.  Print out the histogram of your transformed data using the code below (you should be able to get it to look like a normal distribution without too much work).\n",
    "\n",
    "Make sure to play around with the structure of your network (the number of hidden layers, and number of nodes in each layer), and the learning rate.  Try to get the validation loss to decrease steadily.\n",
    "\n",
    "Answer the following questions (either in Markdown, or in commented code): Which type of data was better for predicting the volume? Would you expect that data type to be better? Why or why not?\n",
    "\n",
    "I have copied the working k_hidden_layer class to this notebook below.\n",
    "\n",
    "Please send any questions to dfurman@uidaho.edu, or use office hours for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.hist(coord_y, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class k_hidden_layer(object):\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.Bs = [np.zeros((1,size)) for size in sizes[1:]]\n",
    "        self.Ws = [np.random.randn(m,n)/np.sqrt(m)\n",
    "                   for m,n in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        Out = np.dot(X,self.Ws[0])+self.Bs[0]\n",
    "        Outs=[Out]\n",
    "        for W, B in zip(self.Ws[1:], self.Bs[1:]):\n",
    "            Out = np.dot(np.maximum(0,Out), W)+B\n",
    "            Outs.append(Out)\n",
    "        if y is None:\n",
    "            return Outs\n",
    "        \n",
    "        P=np.zeros_like(Out)\n",
    "        N=X.shape[0]\n",
    "        P[range(N),y]=1       \n",
    "        M=np.max(Out,axis=1,keepdims=True)\n",
    "        Q = np.exp(Out-M)/np.sum(np.exp(Out-M), axis=1,keepdims=True)\n",
    "        L = -np.mean(np.log(Q[range(N),y]+1e-300)) # Add eps to avoid overflow\n",
    "        for W in self.Ws:\n",
    "            L += reg*np.sum(W**2)\n",
    "            \n",
    "        dWs = [np.zeros_like(W) for W in self.Ws]\n",
    "        dBs = [np.zeros_like(B) for B in self.Bs]\n",
    "        dOuts = [np.zeros_like(Out) for Out in Outs]\n",
    "        dOuts[-1] = (Q-P)/N\n",
    "        for i in range(2,self.num_layers):\n",
    "            dOuts[-i] = (Outs[-i]>0)*np.dot(dOuts[-i+1],self.Ws[-i+1].T)\n",
    "        dBs[0] = np.sum(dOuts[0], axis=0, keepdims=True)\n",
    "        dWs[0] = np.dot(X.T,dOuts[0])+2*reg*self.Ws[0]\n",
    "        for layer in range(1,self.num_layers-1):    \n",
    "            dBs[layer] = np.sum(dOuts[layer], axis=0, keepdims=True)\n",
    "            dWs[layer] = np.dot(np.maximum(0,Outs[layer-1]).T,dOuts[layer])\n",
    "            dWs[layer] += 2*reg*self.Ws[layer]\n",
    "        return L, dBs, dWs\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None,\n",
    "              learning_rate=1e-3, learning_rate_decay=0.99,\n",
    "              reg=5e-3, epochs=5, batch_size=60, verbose=False):\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "        N = X_train.shape[0]\n",
    "        iteration=0\n",
    "        for epoch in range(epochs):\n",
    "            perm=np.arange(N)\n",
    "            np.random.shuffle(perm)\n",
    "            X_batches = [X_train[perm[k:k+batch_size]] \n",
    "                         for k in range(0, N, batch_size)]\n",
    "            y_batches = [y_train[perm[k:k+batch_size]] \n",
    "                         for k in range(0, N, batch_size)]\n",
    "            for X,y in zip(X_batches, y_batches):\n",
    "                L, dBs, dWs = self.loss(X, y ,reg)\n",
    "                self.Ws = [W - learning_rate*dW for W,dW in zip(self.Ws, dWs)]\n",
    "                self.Bs = [B - learning_rate*dB for B,dB in zip(self.Bs, dBs)]\n",
    "                \n",
    "                if iteration % 100 == 0:  \n",
    "                    loss_history.append(L)   \n",
    "                    y_pred = self.predict(X)\n",
    "                    train_acc = np.mean(y_pred==y) \n",
    "                    train_acc_history.append(train_acc)                               \n",
    "                    if X_val is not None:\n",
    "                        y_val_pred = self.predict(X_val)\n",
    "                        val_acc = np.mean(y_val_pred==y_val) \n",
    "                        val_acc_history.append(val_acc)\n",
    "                    if verbose:\n",
    "                        if X_val is not None:\n",
    "                            print('iteration %d: loss %f, train_acc: %f, val_acc: %f' \n",
    "                                  % (iteration, L, train_acc, val_acc))\n",
    "                        else:\n",
    "                            print('iteration %d: loss %f, train_acc: %f' \n",
    "                                  % (iteration, L, train_acc))\n",
    "                iteration+=1\n",
    "            if X_val is not None:\n",
    "                print('Epoch ', epoch+1, 'validation accuracy: ', val_acc)\n",
    "            else:\n",
    "                print('Epoch ', epoch+1, 'completed.')\n",
    "            learning_rate*=learning_rate_decay\n",
    "         \n",
    "        return {'loss_history': loss_history,\n",
    "                'train_acc_history': train_acc_history,\n",
    "                'val_acc_history': val_acc_history}\n",
    "               \n",
    "    def predict(self, X):\n",
    "        y_pred=np.argmax(self.loss(X)[-1],axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
