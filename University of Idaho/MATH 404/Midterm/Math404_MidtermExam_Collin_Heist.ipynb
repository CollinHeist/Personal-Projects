{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Midterm Term Exam </center>\n",
    "\n",
    "<center>(Due on Friday, March 27)\n",
    "   \n",
    "    \n",
    "    \n",
    "In this exam, you are asked to read the paper on Batch Normalization which is available on BBLearn, and write python code in the $k$-hidden layer model. Then, use the model to classify the symbols in the test set in BBLearn. The training set is the same as the one in this year's Data Science Competition avaiable at https://dscomp.nkn.uidaho.edu/\n",
    "        \n",
    "Here I briefly describe batch normalization: Suppose $O_1$, $O_2$, ..., are the output of the $i$-th layer before applying activation function $a$,  \n",
    "$$ O_1= XW_1+B_1;\\ \\ O_i=a(O_{i-1})W_i+B_i,$$ \n",
    "for $i=2,\\ldots, n$. \n",
    "\n",
    "Suppose $O_{i-1}$ is an $N\\times n_i$ matrix. Before applying activation function, we normalize each column vector of $O_{i-1}$ so that the new column vector will have a desired mean and standard deviation. This is done by first  subtracting its mean and devided it by its standard deviation, so that the new column vector will have mean 0 and standard deviation 1. More precisely, if the $j$-th column vector is vector $o_j^{(i-1)}$. The corresponding  new column is then\n",
    "$$\\widetilde{o}_j^{(i-1)}=\\frac{o_j^{(i-1)}-np.mean(o_j^{(i-1)})}{\\sqrt{np.var(o_j^{(i-1)})+\\varepsilon}},$$\n",
    "where $\\varepsilon$ is a small positive number called smooth factor to prevent from dividing by 0. Denote \n",
    "<span style=\"color:blue\">\n",
    "$$\\widetilde{O}^{(i-1)}=(\\widetilde{o}_ j^{(i-1)}).$$\n",
    "</span>\n",
    "\n",
    "While the above step is probably not much inovation, the batch normalization proposed by Ioffe and Szegedy in 2015 takes further a creative step: scaling and shifting $\\widetilde{o}_j^{(i-1)}$'s using learnable parameters!\n",
    "\n",
    "$$\\widehat{o}_j^{(i-1)}= \\gamma_j^{(i-1)}\\cdot \\widetilde{o}_j^{(i-1)}+\\beta_j^{(i-1)}=\\gamma_j^{(i-1)}\\cdot \\frac{v_j^{(i-1)}-np.mean(o_j^{(i-1)})}{\\sqrt{np.var(o_j^{(i-1)})+\\varepsilon}}+\\beta_j^{(i-1)}. $$\n",
    "Thus, if we let \n",
    "<span style=\"color:blue\">\n",
    "$$\\widehat{O}^{(i-1)}=(\\widehat{o}_j^{(i-1)})=\\Gamma^{(i-1)}\\otimes \\widetilde{O}^{(i-1)}+\\beta^{(i-1)},$$\n",
    "</span>\n",
    "where \n",
    "$$\\Gamma^{(i-1)}=\\begin{bmatrix}\\gamma_1^{(i-1)}&\\gamma_2^{(i-1)}&\\cdots&\\gamma_{n_i}^{(i-1)}\\\\\n",
    "    \\gamma_1^{(i-1)}&\\gamma_2^{(i-1)}&\\cdots&\\gamma_{n_i}^{(i-1)}\\\\\n",
    "    \\vdots&\\vdots&\\cdots&\\vdots\\\\\n",
    "    \\gamma_1^{(i-1)}&\\gamma_2^{(i-1)}&\\cdots&\\gamma_{n_i}^{(i-1)}\n",
    "\\end{bmatrix}=\\begin{bmatrix}1\\\\1\\\\\\vdots\\\\1\n",
    "\\end{bmatrix}\\begin{bmatrix}\\gamma_1^{(i-1)}&\\gamma_2^{(i-1)}&\\cdots&\\gamma_{n_i}^{(i-1)}\n",
    "\\end{bmatrix},$$\n",
    "$$\\beta^{(i-1)}=\\begin{bmatrix}\\beta_1^{(i-1)}&\\beta_2^{(i-1)}&\\cdots&\\beta_{n_i}^{(i-1)}\\\\\n",
    "    \\beta_1^{(i-1)}&\\beta_2^{(i-1)}&\\cdots&\\beta_{n_i}^{(i-1)}\\\\\n",
    "    \\vdots&\\vdots&\\cdots&\\vdots\\\\\n",
    "    \\beta_1^{(i-1)}&\\beta_2^{(i-1)}&\\cdots&\\beta_{n_i}^{(i-1)}\n",
    "\\end{bmatrix}=\\begin{bmatrix}1\\\\1\\\\\\vdots\\\\1\n",
    "\\end{bmatrix}\\begin{bmatrix}\\beta_1^{(i-1)}&\\beta_2^{(i-1)}&\\cdots&\\beta_{n_i}^{(i-1)}\n",
    "\\end{bmatrix},$$ we can write\n",
    "<span style=\"color:blue\">\n",
    "$$ O_1=XW_1,\\ \\ \\ \\ O_i=a\\left(\\widehat{O}^{(i-1)}\\right)W_i, \\ i=2,3,\\ldots, n.$$ \n",
    "</span>\n",
    "(Note: there is no need for the original biases $B_i$.) Because of these learnable scaling and shifting parameteres $\\gamma_{i-1}$ and $\\beta_{i-1}$, neural network performance is less dependent on weights initialization and regularization. Note that using numpy default, we can code $\\Gamma^{(i-1)}=[[\\gamma_1^{(i-1)}, \\gamma_2^{(i-1)},\\ldots, \\gamma_{n_i}^{(i-1)}]]$ and $\\beta^{(i-1)}=[[\\beta_1^{(i-1)}, \\beta_2^{(i-1)},\\ldots, \\beta_{n_i}^{(i-1)}]]$ as shape $1\\times n_i$. However, when doing gradient calculation, we need to know that the exact expression is $$\\Gamma^{(i-1)}={\\mathbb 1}[[\\gamma_1^{(i-1)}, \\gamma_2^{(i-1)},\\ldots, \\gamma_{n_i}^{(i-1)}]],$$\n",
    "$$\\beta^{(i-1)}={\\mathbb 1}[[\\beta_1^{(i-1)}, \\beta_2^{(i-1)},\\ldots, \\beta_{n_i}^{(i-1)}]],$$\n",
    "    where ${\\mathbb 1}=np.ones((N,n_i))$,\n",
    "\n",
    "The rest of the neural network may stay the same.\n",
    "    \n",
    "Recall that the main gradient formulas\n",
    "  \n",
    "$\\frac{\\partial L}{\\partial U}=\\frac{\\partial L}{\\partial a(U)}\\otimes (U>0)$  \n",
    "    \n",
    "$\\frac{\\partial L}{\\partial U}=\\frac{\\partial L}{\\partial (U\\otimes A)}\\otimes A$  \n",
    "    \n",
    "$\\frac{\\partial L}{\\partial U}=\\frac{\\partial L}{\\partial (U+A)}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial U}=A^T\\frac{\\partial L}{\\partial (AU)}$\n",
    "  \n",
    "$\\frac{\\partial L}{\\partial U}=\\frac{\\partial L}{\\partial (UA)}A^T$\n",
    "    \n",
    "From these formulas, we can obtain for example\n",
    "   \n",
    "$\\frac{\\partial L}{\\partial W_i}=a(\\widehat{O}^{(i-1)})^T\\frac{\\partial L}{\\partial O_i}$\n",
    "    \n",
    "$\\frac{\\partial L}{\\partial \\widehat{O}^{(i-1)}}=\\frac{\\partial L}{\\partial a(\\widehat{O}^{(i-1)})}\\otimes (\\widehat{O}^{(i-1)}>0)=\\left(\\frac{\\partial L}{\\partial O_{i}}W_i^T\\right)\\otimes (\\widehat{O}^{(i-1)}>0)$\n",
    "\n",
    "Using the second expression above, you can get $\\frac{\\partial L}{\\partial \\Gamma^{(i-1)}}$ and $\\frac{\\partial L}{\\partial \\beta^{(i-1)}}$. \n",
    "    \n",
    "Finish the codes below, and then use it to classify the attached testset. The exam will be graded by both the correctness of the coding and prediction accuracy. \n",
    "    \n",
    "Optional: You may receive extra credits if you also implement other activation functions, or other optimization methods.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Subidivide a Dataset into Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Basic number package\n",
    "import traceback   # File-traceback-tracing\n",
    "import pickle      # Storing object instances to file\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "def subdivide_data(x, y, test_percentage=0.025):\n",
    "    assert len(x) == len(y), \"Lengths of x and y must be equal.\"\n",
    "    indices = np.arange(len(x))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    return {\"X-Train\": x[indices][:int(len(x) * (1.0 - test_percentage))], \"Y-Train\": y[indices][:int(len(x) * (1.0 - test_percentage))],\n",
    "            \"X-Test\": x[indices][-int(len(x) * test_percentage):], \"Y-Test\": y[indices][-int(len(x) * test_percentage):]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of my Activation Function\n",
    "\n",
    "### I am using a Leaky ReLU Function for this network\n",
    "\n",
    "$$a(x)=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "    0.1x & x\\leq 0 \\\\\n",
    "    x & x>0\n",
    "\\end{array}\n",
    "\\right.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_function = lambda x: x if x > 0 else 0.1 * x\n",
    "back_activation_function = lambda x: 1 if x > 0 else 0.1\n",
    "\n",
    "activation_function = np.vectorize(activation_function)\n",
    "back_activation_function = np.vectorize(back_activation_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Hidden Layer Neural Network Class\n",
    "\n",
    "### I added layer-by-layer dropout to the Network (the second parameter during the `init`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class k_hidden_layer_bn(object):\n",
    "    def __init__(self, sizes, dropouts=None):\n",
    "        self.object_name = traceback.extract_stack()[-2][3][:traceback.extract_stack()[-2][3].find('=')].strip() # Name of the instance of this class object\n",
    "        self.num_layers = len(sizes)        \n",
    "        self.weights = [np.random.randn(m, n) / np.sqrt(m) for m, n in zip(sizes[:-1], sizes[1:])]\n",
    "        self.gammas = [np.ones((1, size)) for size in sizes[1:-1]] # Batch Normalization 'weight'\n",
    "        self.betas = [np.zeros((1, size)) for size in sizes[1:-1]] # Batch Normalization 'bias'\n",
    "        if dropouts == None:\n",
    "            self.dropouts = [1.0 for layer in range(len(sizes))]\n",
    "        else:\n",
    "            self.dropouts = dropouts\n",
    "        \n",
    "    def predict(self, X, v=False):\n",
    "        if v:\n",
    "            print (self.loss(X))\n",
    "        return np.argmax(self.loss(X), axis=1)\n",
    "        \n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        if y is None: # When predicting, don't dropout any nodes\n",
    "            output = np.dot(X, self.weights[0])\n",
    "        else:\n",
    "            output = np.dot(X, self.weights[0] * ((np.random.uniform(0, 1.0, (self.weights[0].shape[0], 1)) > self.dropouts[0]) * 1.0))\n",
    "        layer_out_list, cache_list, y_i_list = [output], [], []\n",
    "        # Inputs -> Weights -> [Normalization -> Activation -> Weights] -> Output\n",
    "        for i in range(self.num_layers - 2):\n",
    "            x_min_mu = output - np.mean(output, axis=0, keepdims=True)\n",
    "            variance = np.var(output, axis=0, keepdims=True) + 1e-8\n",
    "            x_hat = x_min_mu / np.sqrt(variance)\n",
    "            y_i = x_hat * self.gammas[i] + self.betas[i]\n",
    "            \n",
    "            if y is None: # When predicting, don't implement dropouts\n",
    "                output = np.dot(activation_function(y_i), self.weights[i + 1])\n",
    "            else:\n",
    "                output = np.dot(activation_function(y_i), self.weights[i + 1] * ((np.random.uniform(0, 1.0, (self.weights[i + 1].shape[0], 1)) > self.dropouts[i + 1]) * 1.0))\n",
    "            layer_out_list.append(output); cache_list.append((x_hat, x_min_mu, variance)); y_i_list.append(y_i)\n",
    "            \n",
    "        if y is None: # Good\n",
    "            return np.exp(output - np.max(output)) / np.max(output)\n",
    "\n",
    "        N = X.shape[0]\n",
    "        P = np.zeros_like(output)\n",
    "        P[range(N), y] = 1.0\n",
    "        M = np.max(output, axis=1, keepdims=True)\n",
    "        Q = np.exp(output - M) / np.sum(np.exp(output - M), axis=1, keepdims=True)\n",
    "        loss = -np.mean(np.log(Q[range(N), y] + 1e-300))\n",
    "        for W in self.weights:\n",
    "            loss += reg * np.sum(W ** 2)\n",
    "\n",
    "        d_gammas = [np.zeros_like(gamma) for gamma in self.gammas]\n",
    "        d_betas = [np.zeros_like(beta) for beta in self.betas]\n",
    "        d_outs = [np.zeros_like(o) for o in layer_out_list]\n",
    "        \n",
    "        d_outs[-1] = (Q - P) / N\n",
    "        for i in range(2, self.num_layers):\n",
    "            x_hat, x_min_mu, variance = cache_list[-i + 1]\n",
    "            dl_dy = back_activation_function(np.dot(d_outs[-i + 1], self.weights[-i + 1].T)) # Back propagate through (activation <- Weights <- Output[next stage])\n",
    "            \n",
    "            # Intermediate variables for computing the back propogations\n",
    "            dxhat = dl_dy * np.ones((N, self.gammas[-i + 1].shape[1])) * self.gammas[-i + 1]\n",
    "            dsqrtvar = -1 / variance * np.sum(dxhat * x_min_mu, axis=0)\n",
    "            dsq = np.ones((N, self.gammas[-i + 1].shape[1])) / N * 0.5 / np.sqrt(variance) * dsqrtvar\n",
    "            dxmu2 = 2 * x_min_mu * dsq\n",
    "            dmu = -np.sum((dxhat / variance) + dxmu2, axis=0)\n",
    "            dx2 = np.ones((N, self.gammas[-i + 1].shape[1])) * dmu / N\n",
    "            \n",
    "            # Compute the back-propogation of dL/dy, dL/dgamma, and dL/dbeta\n",
    "            d_outs[-i] = (dxhat / variance) + dxmu2 + dx2\n",
    "            d_gammas[-i + 1] = np.sum(dl_dy * x_hat, axis=0)\n",
    "            d_betas[-i + 1] = np.sum(dl_dy, axis=0)\n",
    "            \n",
    "        # Update the weights based on the back-propagated dL/dy values calculated above\n",
    "        d_weights = [np.zeros_like(W) for W in self.weights]\n",
    "        d_weights[0] = np.dot(X.T, d_outs[0]) + 2 * reg * self.weights[0]\n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            d_weights[i] = np.dot((y_i_list[i - 1]).T, d_outs[i]) + 2 * reg * self.weights[i]\n",
    "\n",
    "        return loss, d_weights, d_gammas, d_betas\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, learning_rate=1e-3, learning_rate_decay=0.99, reg=5e-3, epochs=5, batch_size=50, verbose=False):\n",
    "        loss_history, train_acc_history, val_acc_history = [], [], []\n",
    "        N = X_train.shape[0]\n",
    "        iteration = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            perm = np.arange(N)\n",
    "            np.random.shuffle(perm)\n",
    "            X_batches = [X_train[perm[k:k+batch_size]] for k in range(0, N, batch_size)]\n",
    "            y_batches = [y_train[perm[k:k+batch_size]] for k in range(0, N, batch_size)]\n",
    "            for X, y in zip(X_batches, y_batches):\n",
    "                L, dWs, dgammas, dbetas = self.loss(X, y, reg)\n",
    "                self.weights = [W - learning_rate * dW for W, dW in zip(self.weights, dWs)]\n",
    "                self.gammas = [gamma - learning_rate * dgamma for gamma, dgamma in zip(self.gammas, dgammas)]\n",
    "                self.betas = [beta - learning_rate * dbeta for beta, dbeta in zip(self.betas, dbetas)]\n",
    "                \n",
    "                if iteration % 50 == 0:  \n",
    "                    loss_history.append(L)   \n",
    "                    y_pred = self.predict(X)\n",
    "                    train_acc = np.mean(y_pred == y)\n",
    "                    train_acc_history.append(train_acc)\n",
    "                    \n",
    "                    if X_val is not None:\n",
    "                        y_val_pred = self.predict(X_val)\n",
    "                        val_acc = np.mean(y_val_pred == y_val) \n",
    "                        val_acc_history.append(val_acc)\n",
    "                        \n",
    "                    if verbose and X_val is not None:\n",
    "                        print('\\n\\riteration %d: loss %f, train_acc: %f, val_acc: %f'  % (iteration, L, train_acc, val_acc), end='')\n",
    "                    elif verbose:\n",
    "                        print('\\n\\riteration %d: loss %f, train_acc: %f'% (iteration, L, train_acc), end='')\n",
    "                            \n",
    "                iteration += 1\n",
    "                \n",
    "            if X_val is not None:\n",
    "                print('\\nEpoch ', epoch+1, 'validation accuracy: ', val_acc)\n",
    "            else:\n",
    "                print('\\nEpoch ', epoch+1, 'completed.')\n",
    "                \n",
    "            learning_rate *= learning_rate_decay\n",
    "            \n",
    "        return {'loss_history': loss_history, 'train_acc_history': train_acc_history, 'val_acc_history': val_acc_history}\n",
    "    \n",
    "    def save_to_file(self, name=None):\n",
    "        filename = '{}.pickle'.format(self.object_name) if name is None else '{}.pickle'.format(name)\n",
    "        file = open(filename, 'wb')\n",
    "        pickle.dump(self, file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('/Volumes/Seagate Backup Plus Drive/Miscellaneous/DataScience2020-Data/train_data.npy').reshape(8000, -1)\n",
    "train_labels = np.load('/Volumes/Seagate Backup Plus Drive/Miscellaneous/DataScience2020-Data/train_labels.npy')\n",
    "test_data = np.loadtxt('test_data.npy', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 0: loss 23.698539, train_acc: 1.000000, val_acc: 0.480000\n",
      "iteration 50: loss 23.195534, train_acc: 0.750000, val_acc: 0.705000\n",
      "iteration 100: loss 23.382095, train_acc: 0.781250, val_acc: 0.712500\n",
      "iteration 150: loss 24.334114, train_acc: 0.687500, val_acc: 0.702500\n",
      "iteration 200: loss 24.778296, train_acc: 0.718750, val_acc: 0.727500\n",
      "Epoch  1 validation accuracy:  0.7275\n",
      "\n",
      "iteration 250: loss 25.211148, train_acc: 0.656250, val_acc: 0.725000\n",
      "iteration 300: loss 25.644233, train_acc: 0.687500, val_acc: 0.732500\n",
      "iteration 350: loss 25.850889, train_acc: 0.718750, val_acc: 0.732500\n",
      "iteration 400: loss 26.864928, train_acc: 0.593750, val_acc: 0.745000\n",
      "iteration 450: loss 26.470813, train_acc: 0.625000, val_acc: 0.745000\n",
      "Epoch  2 validation accuracy:  0.745\n",
      "\n",
      "iteration 500: loss 26.513603, train_acc: 0.812500, val_acc: 0.737500\n",
      "iteration 550: loss 27.262367, train_acc: 0.656250, val_acc: 0.752500\n",
      "iteration 600: loss 27.375857, train_acc: 0.656250, val_acc: 0.755000\n",
      "iteration 650: loss 27.769185, train_acc: 0.718750, val_acc: 0.755000\n",
      "iteration 700: loss 27.515182, train_acc: 0.906250, val_acc: 0.750000\n",
      "Epoch  3 validation accuracy:  0.75\n",
      "\n",
      "iteration 750: loss 28.016929, train_acc: 0.750000, val_acc: 0.757500\n",
      "iteration 800: loss 28.215335, train_acc: 0.812500, val_acc: 0.757500\n",
      "iteration 850: loss 28.621495, train_acc: 0.812500, val_acc: 0.755000\n",
      "iteration 900: loss 28.620940, train_acc: 0.906250, val_acc: 0.765000\n",
      "iteration 950: loss 28.893867, train_acc: 0.843750, val_acc: 0.762500\n",
      "Epoch  4 validation accuracy:  0.7625\n",
      "\n",
      "iteration 1000: loss 29.114914, train_acc: 0.812500, val_acc: 0.772500\n",
      "iteration 1050: loss 29.680066, train_acc: 0.812500, val_acc: 0.775000\n",
      "iteration 1100: loss 29.646312, train_acc: 0.906250, val_acc: 0.772500\n",
      "iteration 1150: loss 29.991696, train_acc: 0.812500, val_acc: 0.757500\n",
      "Epoch  5 validation accuracy:  0.7575\n",
      "\n",
      "iteration 1200: loss 30.284197, train_acc: 0.812500, val_acc: 0.785000\n",
      "iteration 1250: loss 30.414548, train_acc: 0.843750, val_acc: 0.782500\n",
      "iteration 1300: loss 30.890943, train_acc: 0.718750, val_acc: 0.767500\n",
      "iteration 1350: loss 30.830226, train_acc: 0.843750, val_acc: 0.770000\n",
      "iteration 1400: loss 31.237064, train_acc: 0.781250, val_acc: 0.777500\n",
      "Epoch  6 validation accuracy:  0.7775\n",
      "\n",
      "iteration 1450: loss 31.211056, train_acc: 0.843750, val_acc: 0.765000\n",
      "iteration 1500: loss 31.660839, train_acc: 0.781250, val_acc: 0.757500\n",
      "iteration 1550: loss 31.826254, train_acc: 0.781250, val_acc: 0.770000\n",
      "iteration 1600: loss 32.006343, train_acc: 0.875000, val_acc: 0.760000\n",
      "iteration 1650: loss 32.101683, train_acc: 0.812500, val_acc: 0.762500\n",
      "Epoch  7 validation accuracy:  0.7625\n",
      "\n",
      "iteration 1700: loss 32.714757, train_acc: 0.750000, val_acc: 0.760000\n",
      "iteration 1750: loss 32.651104, train_acc: 0.750000, val_acc: 0.775000\n",
      "iteration 1800: loss 32.999652, train_acc: 0.812500, val_acc: 0.770000\n",
      "iteration 1850: loss 33.267621, train_acc: 0.687500, val_acc: 0.767500\n",
      "iteration 1900: loss 33.216319, train_acc: 0.750000, val_acc: 0.772500\n",
      "Epoch  8 validation accuracy:  0.7725\n",
      "\n",
      "iteration 1950: loss 33.810726, train_acc: 0.843750, val_acc: 0.770000\n",
      "iteration 2000: loss 33.559915, train_acc: 0.812500, val_acc: 0.772500\n",
      "iteration 2050: loss 33.751864, train_acc: 0.875000, val_acc: 0.760000\n",
      "iteration 2100: loss 33.838531, train_acc: 0.875000, val_acc: 0.775000\n",
      "Epoch  9 validation accuracy:  0.775\n",
      "\n",
      "iteration 2150: loss 34.132138, train_acc: 0.875000, val_acc: 0.777500\n",
      "iteration 2200: loss 34.097619, train_acc: 0.937500, val_acc: 0.770000\n",
      "iteration 2250: loss 35.042774, train_acc: 0.718750, val_acc: 0.765000\n",
      "iteration 2300: loss 34.750473, train_acc: 0.875000, val_acc: 0.775000\n",
      "iteration 2350: loss 34.887475, train_acc: 0.812500, val_acc: 0.770000\n",
      "Epoch  10 validation accuracy:  0.77\n",
      "\n",
      "iteration 2400: loss 35.218742, train_acc: 0.750000, val_acc: 0.767500\n",
      "iteration 2450: loss 35.453370, train_acc: 0.781250, val_acc: 0.767500\n",
      "iteration 2500: loss 35.535523, train_acc: 0.875000, val_acc: 0.770000\n",
      "iteration 2550: loss 35.756570, train_acc: 0.937500, val_acc: 0.762500\n",
      "iteration 2600: loss 36.031526, train_acc: 0.812500, val_acc: 0.750000\n",
      "Epoch  11 validation accuracy:  0.75\n",
      "\n",
      "iteration 2650: loss 36.185855, train_acc: 0.843750, val_acc: 0.775000\n",
      "iteration 2700: loss 36.590097, train_acc: 0.750000, val_acc: 0.775000\n",
      "iteration 2750: loss 36.549734, train_acc: 0.812500, val_acc: 0.770000\n",
      "iteration 2800: loss 37.593040, train_acc: 0.687500, val_acc: 0.762500\n",
      "iteration 2850: loss 36.878150, train_acc: 0.906250, val_acc: 0.762500\n",
      "Epoch  12 validation accuracy:  0.7625\n",
      "\n",
      "iteration 2900: loss 37.840720, train_acc: 0.687500, val_acc: 0.777500\n",
      "iteration 2950: loss 37.548634, train_acc: 0.812500, val_acc: 0.775000\n",
      "iteration 3000: loss 37.508000, train_acc: 0.875000, val_acc: 0.755000\n",
      "iteration 3050: loss 38.157135, train_acc: 0.812500, val_acc: 0.757500\n",
      "Epoch  13 validation accuracy:  0.7575\n",
      "\n",
      "iteration 3100: loss 37.968872, train_acc: 0.875000, val_acc: 0.755000\n",
      "iteration 3150: loss 38.323867, train_acc: 0.781250, val_acc: 0.770000\n",
      "iteration 3200: loss 38.650512, train_acc: 0.812500, val_acc: 0.757500\n",
      "iteration 3250: loss 38.576315, train_acc: 0.875000, val_acc: 0.762500\n",
      "iteration 3300: loss 38.979757, train_acc: 0.750000, val_acc: 0.770000\n",
      "Epoch  14 validation accuracy:  0.77\n",
      "\n",
      "iteration 3350: loss 38.869809, train_acc: 0.875000, val_acc: 0.757500\n",
      "iteration 3400: loss 39.146975, train_acc: 0.843750, val_acc: 0.765000\n",
      "iteration 3450: loss 39.434767, train_acc: 0.875000, val_acc: 0.767500\n",
      "iteration 3500: loss 39.871809, train_acc: 0.875000, val_acc: 0.770000\n",
      "iteration 3550: loss 40.180086, train_acc: 0.812500, val_acc: 0.765000\n",
      "Epoch  15 validation accuracy:  0.765\n",
      "\n",
      "iteration 3600: loss 40.226005, train_acc: 0.812500, val_acc: 0.767500\n",
      "iteration 3650: loss 40.556991, train_acc: 0.718750, val_acc: 0.770000\n",
      "iteration 3700: loss 40.772075, train_acc: 0.843750, val_acc: 0.767500\n",
      "iteration 3750: loss 40.726642, train_acc: 0.875000, val_acc: 0.752500\n",
      "iteration 3800: loss 40.949091, train_acc: 0.875000, val_acc: 0.780000\n",
      "Epoch  16 validation accuracy:  0.78\n",
      "\n",
      "iteration 3850: loss 41.199770, train_acc: 0.875000, val_acc: 0.757500\n",
      "iteration 3900: loss 41.356419, train_acc: 0.812500, val_acc: 0.772500\n",
      "iteration 3950: loss 41.393529, train_acc: 0.937500, val_acc: 0.757500\n",
      "iteration 4000: loss 42.114124, train_acc: 0.687500, val_acc: 0.757500\n",
      "Epoch  17 validation accuracy:  0.7575\n",
      "\n",
      "iteration 4050: loss 41.792576, train_acc: 0.937500, val_acc: 0.772500\n",
      "iteration 4100: loss 42.738129, train_acc: 0.718750, val_acc: 0.770000\n",
      "iteration 4150: loss 42.900026, train_acc: 0.843750, val_acc: 0.760000\n",
      "iteration 4200: loss 42.591065, train_acc: 0.812500, val_acc: 0.762500\n",
      "iteration 4250: loss 42.961698, train_acc: 0.906250, val_acc: 0.850000\n",
      "Epoch  18 validation accuracy:  0.75\n",
      "\n",
      "iteration 4300: loss 43.325258, train_acc: 0.781250, val_acc: 0.755000\n",
      "iteration 4350: loss 43.217486, train_acc: 0.875000, val_acc: 0.820000\n",
      "iteration 4400: loss 43.939616, train_acc: 0.812500, val_acc: 0.770000\n",
      "iteration 4450: loss 44.293800, train_acc: 0.812500, val_acc: 0.760000\n",
      "iteration 4500: loss 44.504295, train_acc: 0.781250, val_acc: 0.760000\n",
      "Epoch  19 validation accuracy:  0.76\n",
      "\n",
      "iteration 4550: loss 44.221585, train_acc: 0.906250, val_acc: 0.867500\n",
      "iteration 4600: loss 44.602871, train_acc: 0.843750, val_acc: 0.762500\n",
      "iteration 4650: loss 44.649613, train_acc: 0.937500, val_acc: 0.852500\n",
      "iteration 4700: loss 44.904183, train_acc: 0.937500, val_acc: 0.867500\n",
      "iteration 4750: loss 45.628036, train_acc: 0.850000, val_acc: 0.772500\n",
      "Epoch  20 validation accuracy:  0.7725\n",
      "\n",
      "iteration 4800: loss 45.156109, train_acc: 0.937500, val_acc: 0.845000\n",
      "iteration 4850: loss 46.055771, train_acc: 0.875000, val_acc: 0.757500\n",
      "iteration 4900: loss 45.966389, train_acc: 0.843750, val_acc: 0.785000\n",
      "iteration 4950: loss 46.153220, train_acc: 0.906250, val_acc: 0.860000\n",
      "Epoch  21 validation accuracy:  0.86\n",
      "\n",
      "iteration 5000: loss 46.399309, train_acc: 0.812500, val_acc: 0.770000\n",
      "iteration 5050: loss 46.314850, train_acc: 0.937500, val_acc: 0.867500\n",
      "iteration 5100: loss 46.634210, train_acc: 0.937500, val_acc: 0.860000\n",
      "iteration 5150: loss 47.099163, train_acc: 0.906250, val_acc: 0.855000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5200: loss 47.399788, train_acc: 0.843750, val_acc: 0.765000\n",
      "Epoch  22 validation accuracy:  0.765\n",
      "\n",
      "iteration 5250: loss 47.794395, train_acc: 0.750000, val_acc: 0.752500\n",
      "iteration 5300: loss 47.691685, train_acc: 0.937500, val_acc: 0.765000\n",
      "iteration 5350: loss 47.734642, train_acc: 0.968750, val_acc: 0.752500\n",
      "iteration 5400: loss 48.397572, train_acc: 0.875000, val_acc: 0.757500\n",
      "iteration 5450: loss 48.560637, train_acc: 0.843750, val_acc: 0.752500\n",
      "Epoch  23 validation accuracy:  0.7525\n",
      "\n",
      "iteration 5500: loss 49.058456, train_acc: 0.843750, val_acc: 0.747500\n",
      "iteration 5550: loss 48.998672, train_acc: 0.875000, val_acc: 0.767500\n",
      "iteration 5600: loss 48.711945, train_acc: 0.887500, val_acc: 0.790000\n",
      "iteration 5650: loss 48.362420, train_acc: 0.906250, val_acc: 0.817500\n",
      "iteration 5700: loss 48.723303, train_acc: 0.916550, val_acc: 0.860000\n",
      "Epoch  24 validation accuracy:  0.86\n",
      "\n",
      "iteration 5750: loss 50.096885, train_acc: 0.943750, val_acc: 0.885000\n",
      "iteration 5800: loss 50.593113, train_acc: 0.912500, val_acc: 0.862500\n",
      "iteration 5850: loss 50.620169, train_acc: 0.906250, val_acc: 0.895000\n",
      "iteration 5900: loss 50.830927, train_acc: 0.947500, val_acc: 0.910000\n",
      "Epoch  25 validation accuracy:  0.9100\n"
     ]
    }
   ],
   "source": [
    "train_dict = subdivide_data(train_data, train_labels, 0.05)\n",
    "network = k_hidden_layer_bn([4096, 4096, 16], [0.05, 0.30, 0.05])\n",
    "history_dict = network.train(train_dict['X-Train'], train_dict['Y-Train'], train_dict['X-Test'], train_dict['Y-Test'],\n",
    "     epochs=32, batch_size=32, verbose=True, learning_rate=1e-4, learning_rate_decay=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final training accuracy I was able to achieve with this model was 94.75% after 25 epochs, with a valiation accuracy of 91%; however if I had trained the network for longer I suspect I could improve this metric.\n",
    "\n",
    "## Make Predictions on the test data\n",
    "\n",
    "### The results are saved to `predictions.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('predictions', [network.predict(val)[0] for val in test_data])\n",
    "network.save_to_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
