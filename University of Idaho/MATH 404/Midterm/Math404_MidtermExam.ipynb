{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Midterm Term Exam </center>\n",
    "\n",
    "<center>(Due on Friday, March 27)\n",
    "   \n",
    "    \n",
    "    \n",
    "In this exam, you are asked to read the paper on Batch Normalization which is available on BBLearn, and write python code in the $k$-hidden layer model. Then, use the model to classify the symbols in the test set in BBLearn. The training set is the same as the one in this year's Data Science Competition avaiable at https://dscomp.nkn.uidaho.edu/\n",
    "        \n",
    "Here I briefly describe batch normalization: Suppose $O_1$, $O_2$, ..., are the output of the $i$-th layer before applying activation function $a$,  \n",
    "$$ O_1= XW_1+B_1;\\ \\ O_i=a(O_{i-1})W_i+B_i,$$ \n",
    "for $i=2,\\ldots, n$. \n",
    "\n",
    "Suppose $O_{i-1}$ is an $N\\times n_i$ matrix. Before applying activation function, we normalize each column vector of $O_{i-1}$ so that the new column vector will have a desired mean and standard deviation. This is done by first  subtracting its mean and devided it by its standard deviation, so that the new column vector will have mean 0 and standard deviation 1. More precisely, if the $j$-th column vector is vector $o_j^{(i-1)}$. The corresponding  new column is then\n",
    "$$\\widetilde{o}_j^{(i-1)}=\\frac{o_j^{(i-1)}-np.mean(o_j^{(i-1)})}{\\sqrt{np.var(o_j^{(i-1)})+\\varepsilon}},$$\n",
    "where $\\varepsilon$ is a small positive number called smooth factor to prevent from dividing by 0. Denote \n",
    "<span style=\"color:blue\">\n",
    "$$\\widetilde{O}^{(i-1)}=(\\widetilde{o}_ j^{(i-1)}).$$\n",
    "</span>\n",
    "\n",
    "While the above step is probably not much inovation, the batch normalization proposed by Ioffe and Szegedy in 2015 takes further a creative step: scaling and shifting $\\widetilde{o}_j^{(i-1)}$'s using learnable parameters!\n",
    "\n",
    "$$\\widehat{o}_j^{(i-1)}= \\gamma_j^{(i-1)}\\cdot \\widetilde{o}_j^{(i-1)}+\\beta_j^{(i-1)}=\\gamma_j^{(i-1)}\\cdot \\frac{v_j^{(i-1)}-np.mean(o_j^{(i-1)})}{\\sqrt{np.var(o_j^{(i-1)})+\\varepsilon}}+\\beta_j^{(i-1)}. $$\n",
    "Thus, if we let \n",
    "<span style=\"color:blue\">\n",
    "$$\\widehat{O}^{(i-1)}=(\\widehat{o}_j^{(i-1)})=\\Gamma^{(i-1)}\\otimes \\widetilde{O}^{(i-1)}+\\beta^{(i-1)},$$\n",
    "</span>\n",
    "where \n",
    "$$\\Gamma^{(i-1)}=\\begin{bmatrix}\\gamma_1^{(i-1)}&\\gamma_2^{(i-1)}&\\cdots&\\gamma_{n_i}^{(i-1)}\\\\\n",
    "    \\gamma_1^{(i-1)}&\\gamma_2^{(i-1)}&\\cdots&\\gamma_{n_i}^{(i-1)}\\\\\n",
    "    \\vdots&\\vdots&\\cdots&\\vdots\\\\\n",
    "    \\gamma_1^{(i-1)}&\\gamma_2^{(i-1)}&\\cdots&\\gamma_{n_i}^{(i-1)}\n",
    "\\end{bmatrix}=\\begin{bmatrix}1\\\\1\\\\\\vdots\\\\1\n",
    "\\end{bmatrix}\\begin{bmatrix}\\gamma_1^{(i-1)}&\\gamma_2^{(i-1)}&\\cdots&\\gamma_{n_i}^{(i-1)}\n",
    "\\end{bmatrix},$$\n",
    "$$\\beta^{(i-1)}=\\begin{bmatrix}\\beta_1^{(i-1)}&\\beta_2^{(i-1)}&\\cdots&\\beta_{n_i}^{(i-1)}\\\\\n",
    "    \\beta_1^{(i-1)}&\\beta_2^{(i-1)}&\\cdots&\\beta_{n_i}^{(i-1)}\\\\\n",
    "    \\vdots&\\vdots&\\cdots&\\vdots\\\\\n",
    "    \\beta_1^{(i-1)}&\\beta_2^{(i-1)}&\\cdots&\\beta_{n_i}^{(i-1)}\n",
    "\\end{bmatrix}=\\begin{bmatrix}1\\\\1\\\\\\vdots\\\\1\n",
    "\\end{bmatrix}\\begin{bmatrix}\\beta_1^{(i-1)}&\\beta_2^{(i-1)}&\\cdots&\\beta_{n_i}^{(i-1)}\n",
    "\\end{bmatrix},$$ we can write\n",
    "<span style=\"color:blue\">\n",
    "$$ O_1=XW_1,\\ \\ \\ \\ O_i=a\\left(\\widehat{O}^{(i-1)}\\right)W_i, \\ i=2,3,\\ldots, n.$$ \n",
    "</span>\n",
    "(Note: there is no need for the original biases $B_i$.) Because of these learnable scaling and shifting parameteres $\\gamma_{i-1}$ and $\\beta_{i-1}$, neural network performance is less dependent on weights initialization and regularization. Note that using numpy default, we can code $\\Gamma^{(i-1)}=[[\\gamma_1^{(i-1)}, \\gamma_2^{(i-1)},\\ldots, \\gamma_{n_i}^{(i-1)}]]$ and $\\beta^{(i-1)}=[[\\beta_1^{(i-1)}, \\beta_2^{(i-1)},\\ldots, \\beta_{n_i}^{(i-1)}]]$ as shape $1\\times n_i$. However, when doing gradient calculation, we need to know that the exact expression is $$\\Gamma^{(i-1)}={\\mathbb 1}[[\\gamma_1^{(i-1)}, \\gamma_2^{(i-1)},\\ldots, \\gamma_{n_i}^{(i-1)}]],$$\n",
    "$$\\beta^{(i-1)}={\\mathbb 1}[[\\beta_1^{(i-1)}, \\beta_2^{(i-1)},\\ldots, \\beta_{n_i}^{(i-1)}]],$$\n",
    "    where ${\\mathbb 1}=np.ones((N,n_i))$,\n",
    "\n",
    "The rest of the neural network may stay the same.\n",
    "    \n",
    "Recall that the main gradient formulas\n",
    "  \n",
    "$\\frac{\\partial L}{\\partial U}=\\frac{\\partial L}{\\partial a(U)}\\otimes (U>0)$  \n",
    "    \n",
    "$\\frac{\\partial L}{\\partial U}=\\frac{\\partial L}{\\partial (U\\otimes A)}\\otimes A$  \n",
    "    \n",
    "$\\frac{\\partial L}{\\partial U}=\\frac{\\partial L}{\\partial (U+A)}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial U}=A^T\\frac{\\partial L}{\\partial (AU)}$\n",
    "  \n",
    "$\\frac{\\partial L}{\\partial U}=\\frac{\\partial L}{\\partial (UA)}A^T$\n",
    "    \n",
    "From these formulas, we can obtain for example\n",
    "   \n",
    "$\\frac{\\partial L}{\\partial W_i}=a(\\widehat{O}^{(i-1)})^T\\frac{\\partial L}{\\partial O_i}$\n",
    "    \n",
    "$\\frac{\\partial L}{\\partial \\widehat{O}^{(i-1)}}=\\frac{\\partial L}{\\partial a(\\widehat{O}^{(i-1)})}\\otimes (\\widehat{O}^{(i-1)}>0)=\\left(\\frac{\\partial L}{\\partial O_{i}}W_i^T\\right)\\otimes (\\widehat{O}^{(i-1)}>0)$\n",
    "\n",
    "Using the second expression above, you can get $\\frac{\\partial L}{\\partial \\Gamma^{(i-1)}}$ and $\\frac{\\partial L}{\\partial \\beta^{(i-1)}}$. \n",
    "    \n",
    "Finish the codes below, and then use it to classify the attached testset. The exam will be graded by both the correctness of the coding and prediction accuracy. \n",
    "    \n",
    "Optional: You may receive extra credits if you also implement other activation functions, or other optimization methods.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Subidivide a Dataset into Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Basic number package\n",
    "import traceback   # File-traceback-tracing\n",
    "import pickle      # Storing object instances to file\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "def subdivide_data(x, y, test_percentage=0.025):\n",
    "    assert len(x) == len(y), \"Lengths of x and y must be equal.\"\n",
    "    indices = np.arange(len(x))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    return {\"X-Train\": x[indices][:int(len(x) * (1.0 - test_percentage))], \"Y-Train\": y[indices][:int(len(x) * (1.0 - test_percentage))],\n",
    "            \"X-Test\": x[indices][-int(len(x) * test_percentage):], \"Y-Test\": y[indices][-int(len(x) * test_percentage):]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of my Activation Function\n",
    "\n",
    "### I am using a Leaky ReLU Function for this network\n",
    "\n",
    "$$a(x)=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "    0.1x & x\\leq 0 \\\\\n",
    "    x & x>0\n",
    "\\end{array}\n",
    "\\right.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_function = lambda x: x if x > 0 else 0.1 * x\n",
    "back_activation_function = lambda x: 1 if x > 0 else 0.1\n",
    "\n",
    "activation_function = np.vectorize(activation_function)\n",
    "back_activation_function = np.vectorize(back_activation_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Hidden Layer Neural Network Class\n",
    "\n",
    "### I added layer-by-layer dropout to the Network (the second parameter during the `init`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class k_hidden_layer_bn(object):\n",
    "    def __init__(self, sizes, dropouts=None):\n",
    "        self.object_name = traceback.extract_stack()[-2][3][:traceback.extract_stack()[-2][3].find('=')].strip() # Name of the instance of this class object\n",
    "        self.num_layers = len(sizes)        \n",
    "        self.weights = [np.random.randn(m, n) / np.sqrt(m) for m, n in zip(sizes[:-1], sizes[1:])]\n",
    "        self.gammas = [np.ones((1, size)) for size in sizes[1:-1]] # Batch Normalization 'weight'\n",
    "        self.betas = [np.zeros((1, size)) for size in sizes[1:-1]] # Batch Normalization 'bias'\n",
    "        if dropouts == None:\n",
    "            self.dropouts = [1.0 for layer in range(len(sizes))]\n",
    "        else:\n",
    "            self.dropouts = dropouts\n",
    "        \n",
    "    def predict(self, X, v=False):\n",
    "        if v:\n",
    "            print (self.loss(X))\n",
    "        return np.argmax(self.loss(X), axis=1)\n",
    "        \n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        if y is None: # When predicting, don't dropout any nodes\n",
    "            output = np.dot(X, self.weights[0])\n",
    "        else:\n",
    "            output = np.dot(X, self.weights[0] * ((np.random.uniform(0, 1.0, (self.weights[0].shape[0], 1)) > self.dropouts[0]) * 1.0))\n",
    "        layer_out_list, cache_list, y_i_list = [output], [], []\n",
    "        # Inputs -> Weights -> [Normalization -> Activation -> Weights] -> Output\n",
    "        for i in range(self.num_layers - 2):\n",
    "            x_min_mu = output - np.mean(output, axis=0, keepdims=True)\n",
    "            variance = np.var(output, axis=0, keepdims=True) + 1e-8\n",
    "            x_hat = x_min_mu / np.sqrt(variance)\n",
    "            y_i = x_hat * self.gammas[i] + self.betas[i]\n",
    "            \n",
    "            if y is None: # When predicting, don't implement dropouts\n",
    "                output = np.dot(activation_function(y_i), self.weights[i + 1])\n",
    "            else:\n",
    "                output = np.dot(activation_function(y_i), self.weights[i + 1] * ((np.random.uniform(0, 1.0, (self.weights[i + 1].shape[0], 1)) > self.dropouts[i + 1]) * 1.0))\n",
    "            layer_out_list.append(output); cache_list.append((x_hat, x_min_mu, variance)); y_i_list.append(y_i)\n",
    "            \n",
    "        if y is None: # Good\n",
    "            return np.exp(output - np.max(output)) / np.max(output)\n",
    "\n",
    "        N = X.shape[0]\n",
    "        P = np.zeros_like(output)\n",
    "        P[range(N), y] = 1.0\n",
    "        M = np.max(output, axis=1, keepdims=True)\n",
    "        Q = np.exp(output - M) / np.sum(np.exp(output - M), axis=1, keepdims=True)\n",
    "        loss = -np.mean(np.log(Q[range(N), y] + 1e-300))\n",
    "        for W in self.weights:\n",
    "            loss += reg * np.sum(W ** 2)\n",
    "\n",
    "        d_gammas = [np.zeros_like(gamma) for gamma in self.gammas]\n",
    "        d_betas = [np.zeros_like(beta) for beta in self.betas]\n",
    "        d_outs = [np.zeros_like(o) for o in layer_out_list]\n",
    "        \n",
    "        d_outs[-1] = (Q - P) / N\n",
    "        for i in range(2, self.num_layers):\n",
    "            x_hat, x_min_mu, variance = cache_list[-i + 1]\n",
    "            dl_dy = back_activation_function(np.dot(d_outs[-i + 1], self.weights[-i + 1].T)) # Back propagate through (activation <- Weights <- Output[next stage])\n",
    "            \n",
    "            # Intermediate variables for computing the back propogations\n",
    "            dxhat = dl_dy * np.ones((N, self.gammas[-i + 1].shape[1])) * self.gammas[-i + 1]\n",
    "            dsqrtvar = -1 / variance * np.sum(dxhat * x_min_mu, axis=0)\n",
    "            dsq = np.ones((N, self.gammas[-i + 1].shape[1])) / N * 0.5 / np.sqrt(variance) * dsqrtvar\n",
    "            dxmu2 = 2 * x_min_mu * dsq\n",
    "            dmu = -np.sum((dxhat / variance) + dxmu2, axis=0)\n",
    "            dx2 = np.ones((N, self.gammas[-i + 1].shape[1])) * dmu / N\n",
    "            \n",
    "            # Compute the back-propogation of dL/dy, dL/dgamma, and dL/dbeta\n",
    "            d_outs[-i] = (dxhat / variance) + dxmu2 + dx2\n",
    "            d_gammas[-i + 1] = np.sum(dl_dy * x_hat, axis=0)\n",
    "            d_betas[-i + 1] = np.sum(dl_dy, axis=0)\n",
    "            \n",
    "        # Update the weights based on the back-propagated dL/dy values calculated above\n",
    "        d_weights = [np.zeros_like(W) for W in self.weights]\n",
    "        d_weights[0] = np.dot(X.T, d_outs[0]) + 2 * reg * self.weights[0]\n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            d_weights[i] = np.dot((y_i_list[i - 1]).T, d_outs[i]) + 2 * reg * self.weights[i]\n",
    "\n",
    "        return loss, d_weights, d_gammas, d_betas\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, learning_rate=1e-3, learning_rate_decay=0.99, reg=5e-3, epochs=5, batch_size=50, verbose=False):\n",
    "        loss_history, train_acc_history, val_acc_history = [], [], []\n",
    "        N = X_train.shape[0]\n",
    "        iteration = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            perm = np.arange(N)\n",
    "            np.random.shuffle(perm)\n",
    "            X_batches = [X_train[perm[k:k+batch_size]] for k in range(0, N, batch_size)]\n",
    "            y_batches = [y_train[perm[k:k+batch_size]] for k in range(0, N, batch_size)]\n",
    "            for X, y in zip(X_batches, y_batches):\n",
    "                L, dWs, dgammas, dbetas = self.loss(X, y, reg)\n",
    "                self.weights = [W - learning_rate * dW for W, dW in zip(self.weights, dWs)]\n",
    "                self.gammas = [gamma - learning_rate * dgamma for gamma, dgamma in zip(self.gammas, dgammas)]\n",
    "                self.betas = [beta - learning_rate * dbeta for beta, dbeta in zip(self.betas, dbetas)]\n",
    "                \n",
    "                if iteration % 50 == 0:  \n",
    "                    loss_history.append(L)   \n",
    "                    y_pred = self.predict(X)\n",
    "                    train_acc = np.mean(y_pred == y)\n",
    "                    train_acc_history.append(train_acc)\n",
    "                    \n",
    "                    if X_val is not None:\n",
    "                        y_val_pred = self.predict(X_val)\n",
    "                        val_acc = np.mean(y_val_pred == y_val) \n",
    "                        val_acc_history.append(val_acc)\n",
    "                        \n",
    "                    if verbose and X_val is not None:\n",
    "                        print('\\n\\riteration %d: loss %f, train_acc: %f, val_acc: %f'  % (iteration, L, train_acc, val_acc), end='')\n",
    "                    elif verbose:\n",
    "                        print('\\n\\riteration %d: loss %f, train_acc: %f'% (iteration, L, train_acc), end='')\n",
    "                            \n",
    "                iteration+=1\n",
    "                \n",
    "            if X_val is not None:\n",
    "                print('\\nEpoch ', epoch+1, 'validation accuracy: ', val_acc)\n",
    "            else:\n",
    "                print('\\nEpoch ', epoch+1, 'completed.')\n",
    "                \n",
    "            learning_rate *= learning_rate_decay\n",
    "            \n",
    "        return {'loss_history': loss_history, 'train_acc_history': train_acc_history, 'val_acc_history': val_acc_history}\n",
    "    \n",
    "    def save_to_file(self, name=None):\n",
    "        filename = '{}.pickle'.format(self.object_name) if name is None else '{}.pickle'.format(name)\n",
    "        file = open(filename, 'wb')\n",
    "        pickle.dump(self, file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('/Volumes/Seagate Backup Plus Drive/Miscellaneous/DataScience2020-Data/train_data.npy').reshape(8000, -1)\n",
    "train_labels = np.load('/Volumes/Seagate Backup Plus Drive/Miscellaneous/DataScience2020-Data/train_labels.npy')\n",
    "test_data = np.loadtxt('test_data.npy', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 0: loss 13.119193, train_acc: 0.953125, val_acc: 0.460000\n",
      "iteration 100: loss 17.798836, train_acc: 0.765625, val_acc: 0.702500\n",
      "Epoch  1 validation accuracy:  0.7025\n",
      "\n",
      "iteration 200: loss 17.582096, train_acc: 0.843750, val_acc: 0.717500\n",
      "Epoch  2 validation accuracy:  0.7175\n",
      "\n",
      "iteration 300: loss 18.165887, train_acc: 0.843750, val_acc: 0.730000\n",
      "Epoch  3 validation accuracy:  0.73\n",
      "\n",
      "iteration 400: loss 18.590751, train_acc: 0.718750, val_acc: 0.732500\n",
      "Epoch  4 validation accuracy:  0.7325\n",
      "\n",
      "iteration 500: loss 18.909615, train_acc: 0.734375, val_acc: 0.727500\n",
      "Epoch  5 validation accuracy:  0.7275\n",
      "\n",
      "iteration 600: loss 18.886856, train_acc: 0.781250, val_acc: 0.740000\n",
      "iteration 700: loss 19.106629, train_acc: 0.765625, val_acc: 0.732500\n",
      "Epoch  6 validation accuracy:  0.7325\n",
      "\n",
      "iteration 800: loss 19.274452, train_acc: 0.750000, val_acc: 0.735000\n",
      "Epoch  7 validation accuracy:  0.735\n",
      "\n",
      "iteration 900: loss 19.228977, train_acc: 0.734375, val_acc: 0.737500\n",
      "Epoch  8 validation accuracy:  0.7375\n",
      "\n",
      "iteration 1000: loss 19.311549, train_acc: 0.750000, val_acc: 0.737500\n",
      "Epoch  9 validation accuracy:  0.7375\n",
      "\n",
      "iteration 1100: loss 19.283502, train_acc: 0.843750, val_acc: 0.740000\n",
      "Epoch  10 validation accuracy:  0.74\n",
      "\n",
      "iteration 1200: loss 19.250052, train_acc: 0.812500, val_acc: 0.737500\n",
      "iteration 1300: loss 19.372316, train_acc: 0.796875, val_acc: 0.737500\n",
      "Epoch  11 validation accuracy:  0.7375\n",
      "\n",
      "iteration 1400: loss 19.322841, train_acc: 0.765625, val_acc: 0.737500\n",
      "Epoch  12 validation accuracy:  0.7375\n",
      "\n",
      "iteration 1500: loss 19.459005, train_acc: 0.765625, val_acc: 0.737500\n",
      "Epoch  13 validation accuracy:  0.7375\n",
      "\n",
      "iteration 1600: loss 19.262777, train_acc: 0.812500, val_acc: 0.737500\n",
      "Epoch  14 validation accuracy:  0.7375\n",
      "\n",
      "iteration 1700: loss 19.261903, train_acc: 0.843750, val_acc: 0.737500\n",
      "Epoch  15 validation accuracy:  0.7375\n",
      "\n",
      "iteration 1800: loss 19.121842, train_acc: 0.843750, val_acc: 0.737500\n",
      "iteration 1900: loss 19.297287, train_acc: 0.812500, val_acc: 0.737500\n",
      "Epoch  16 validation accuracy:  0.7375\n"
     ]
    }
   ],
   "source": [
    "train_dict = subdivide_data(train_data, train_labels, 0.05)\n",
    "network = k_hidden_layer_bn([4096, 2048, 16], [0.1, 0.25, 0.1])\n",
    "history_dict = network.train(train_dict['X-Train'], train_dict['Y-Train'], train_dict['X-Test'], train_dict['Y-Test'],\n",
    "     epochs=16, batch_size=64, verbose=True, learning_rate=5e-4, learning_rate_decay=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final training accuracy I was able to achieve with this model was 81.25% after 16 epochs, with a valiation accuracy of 73.75%; however if I had trained the network for longer I suspect I could improve this metric.\n",
    "\n",
    "## Make Predictions on the test data\n",
    "\n",
    "### The results are saved to `predictions.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('predictions.csv', [network.predict(val)[0] for val in test_data], fmt='%u', delimiter=',')\n",
    "network.save_to_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
